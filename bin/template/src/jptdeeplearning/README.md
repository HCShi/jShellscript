### Description
[https://www.zybuluo.com/hanbingtao/note/433855](零基础入门深度学习 系列课程)
这个里面讲了好多...

### 知识点介绍
``` zsh
y_i 表示输出层节点 i 的输出值
t_i 表示输出层节点 i 的目标值
a_j 表示隐藏层节点 j 的输出值
x_ji 表示节点 i 传递给节点 j 的输入值, 也就是节点 i 的输出值 (将编号大的放前面)
net_j 表示节点 j 的加权输入, 经过类似与 sigmoid() 的变化成为 a_j/y_j

感知机也是特殊的神经元
sigmoid() -> 全连接层神经网络
relu() -> 卷积神经网络

y = sigmoid(x); y\' = y(1 - y); 这个特性很好

常用的训练规则需要的组合: sigmoid 函数、平方和误差、全连接网络、随机梯度下降优化算法
delta_i = y_i * (1 - y_i) * (t_i - y_i)  # 输出层节点反向传播误差
delta_i = a_i * (1 - a_i) * Sigma_k(w_ki * delta_k)

神经元和感知器本质上是一样的, 只不过我们说感知器的时候, 它的激活函数是阶跃函数; 而当我们说神经元时, 激活函数往往选择为 sigmoid 函数或 tanh 函数
激活函数为 y = x 时, 就是一个线性单元了
每个神经元输出的时候都会经过激活函数的计算

神经网络其实就是按照一定规则连接起来的多个神经元. 不同的规则连接起来形成不同的神经网络, eg: 全连接层神经网络, CNN, RNN, GAN
全连接层(full connected, FC) 就是每一个节点都与上一层每个节点连接, 是把前一层的输出特征都综合起来, 所以该层的权值参数是最多的
神经元按照层来布局. 最左边的层叫做输入层; 最右边的层叫输出层. 输入层和输出层之间的层叫做隐藏层, 因为它们对于外部来说是不可见的
输入向量的维度和输入层神经元个数相同, 输出向量的维度和输出层神经元个数相同; 同一层的神经元之间没有连接
输入层的输出没用 sigmoid() 等激活函数; 输出层需要用;

MINST 输出层节点数也是确定的。我们可以用10个节点, 每个节点对应一个分类。输出层10个节点中, 输出最大值的那个节点对应的分类, 就是模型的预测结果

我们知道网络层数越多越好, 也知道层数越多训练难度越大。对于全连接网络, 隐藏层最好不要超过三层

每层都有一个偏置项 bias

前向计算 指的是 向右侧正向计算; 还有后向计算

反向传播算法: 先计算每个节点的误差项, 再计算所有路径的权重; 所有节点的误差项计算完毕后, 才可以更新所有的权重
              误差项的计算顺序必须是从输出层开始, 然后反向依次计算每个隐藏层的误差项, 直到与输入层相连的那个隐藏层

可以说神经网络是一个模型, 那么这些权值就是模型的 **参数**, 也就是模型要学习的东西
然而, 一个神经网络的连接方式、网络的层数、每层的节点数这些参数, 则不是学习出来的, 而是人为事先设置的
对于这些人为设置的参数, 我们称之为 **超参数 (Hyper-Parameters)**

事实上, 一个机器学习算法其实只有两部分
    模型:     从输入特征预测输入的那个函数 y=f(wx)
    目标函数: E(w), 以权重为自变量, 目标函数取最小(最大)值时所对应的权重值, 就是模型的参数的最优值.
              很多时候我们只能获得目标函数的局部最小(最大)值, 因此也只能得到模型参数的局部最优值.
    因此, 如果你想最简洁的介绍一个算法, 列出这两个函数就行了
接下来, 你会用优化算法去求取目标函数的最小(最大)值.
    [随机]梯度{下降|上升}算法就是一个优化算法. 针对同一个目标函数, 不同的优化算法会推导出不同的训练规则. 我们后面还会讲其它的优化算法.
随机梯度下降算法: w_ij = w_ij - rate * E\'(w_ij); 用 偏导 * 学习速率 来作为每次权重更新的量

其实在机器学习中, 算法往往并不是关键, 真正的关键之处在于选取特征. 选取特征需要我们人类对问题的深刻理解, 经验、以及思考.
    而神经网络算法的一个优势, 就在于它能够自动学习到应该提取什么特征, 从而使算法不再那么依赖人类, 而这也是神经网络之所以吸引人的一个方面.
```
### Math
``` zsh
方向导数、偏导数是特殊的全导数
```
